{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonym(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return synonyms\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "error_keywords = {\n",
    "    'does not exist','error','Failed', 'fail', 'failure', 'forbidden', 'critical', 'warning', 'alert',\n",
    "    'password failure', 'connection timeout', 'disk full', 'out of memory',\n",
    "    'segmentation fault', 'not found', 'invalid request', 'service unavailable'\n",
    "}\n",
    "additional_keywords = {\n",
    "    'denied', 'unauthorized', 'invalid', 'timeout', 'abort', 'blocked', \n",
    "    'rejected', 'unreachable', 'not found', 'no such file', 'broken pipe',\n",
    "    'protocol error', 'access violation', 'stack trace', 'core dumped'\n",
    "}\n",
    "\n",
    "#Expand keywords with Synonyms \n",
    "expanded_error_keywords = set(error_keywords)\n",
    "expanded_additional_keywords = set(additional_keywords)\n",
    "\n",
    "for keyword in error_keywords:\n",
    "    expanded_error_keywords.update(get_synonym(keyword))\n",
    "\n",
    "for keyword in additional_keywords:\n",
    "    expanded_additional_keywords.update(get_synonym(keyword))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPANDING ANOMALY RULES\n",
    "\n",
    "def detect_anomaly_extended (message):\n",
    "    message_lower = message.lower()\n",
    "    return any(word in message_lower for word in expanded_error_keywords.union(expanded_additional_keywords))\n",
    "\n",
    "#Regex\n",
    "def detect_pattern_anomalies(message):\n",
    "    suspicious_patterns = [\n",
    "        r'failed\\slogin',    # Failed login attempts\n",
    "        r'sql\\sinjection',   # SQL injection patterns\n",
    "        r'invalid\\suser',    # Invalid user patterns\n",
    "        r'POSSIBLE\\sBREAK-IN\\sATTEMPT',  # Specific break-in attempt message\n",
    "        r'connection\\stimeout',    # Connection timeout patterns\n",
    "        r'disk\\sfull',             # Disk full errors\n",
    "        r'segmentation\\sfault',    # Segmentation fault\n",
    "        r'service\\sunavailable',   # Service unavailable errors\n",
    "        r'protocol\\serror',        # Protocol errors\n",
    "        r'access\\sviolation',      # Access violation errors\n",
    "        r'authentication\\sfailed', # Authentication failure patterns\n",
    "        r'unable\\sto\\sconnect',    # Connection issues\n",
    "        r'login\\sattempt\\sfailure' # Login attempt failures\n",
    "    ]\n",
    "    return any(re.search(pattern, message.lower()) for pattern in suspicious_patterns)\n",
    "\n",
    "#Statistical outliers\n",
    "def detect_statistical_outliers(df):\n",
    "    message_counts = df['message'].value_counts()\n",
    "    z_scores = zscore(message_counts)\n",
    "    outliers = np.abs(z_scores) > 2.5\n",
    "    df['statistical_anomaly'] = df['message'].map(lambda x: message_counts[x] if x in message_counts.index else 0)\n",
    "    df['statistical_anomaly'] = df['statistical_anomaly'].map(lambda x: outliers[message_counts.index.get_loc(x)] if x in message_counts.index else False)\n",
    "\n",
    "\n",
    "# Clustering anomalies using Word2Vec\n",
    "def detect_clustering_anomalies(df):\n",
    "    tokenized_messages = df['message'].apply(lambda msg: msg.lower().split())\n",
    "    word2vec_model = Word2Vec(sentences=tokenized_messages, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "    df['word2vec_vector'] = df['message'].apply(lambda msg: np.mean([word2vec_model.wv[token] for token in msg.lower().split() if token in word2vec_model.wv], axis=0))\n",
    "    \n",
    "    word2vec_features = np.vstack(df['word2vec_vector'].values)\n",
    "    clustering = DBSCAN(eps=0.3, min_samples=5, metric='euclidean').fit(word2vec_features)\n",
    "    df['cluster'] = clustering.labels_\n",
    "    df['clustering_anomaly'] = df['cluster'] == -1\n",
    "    #Drop the word2vec\n",
    "    df.drop(columns=['word2vec_vector'], inplace=True)\n",
    "\n",
    "\n",
    "#Contextual and sequantial analysis \n",
    "def detect_sequential_anomalies(df, window_size=5, threshold=5):\n",
    "    anomalies = df['combined_anomaly'].rolling(window=window_size).sum()\n",
    "    df['sequential_anomaly'] = anomalies >= threshold\n",
    "\n",
    "\n",
    "#APPLY\n",
    "def apply_anomaly_detection(df):\n",
    "    df['keyword_anomaly'] = df['message'].apply(detect_anomaly_extended)\n",
    "    df['pattern_anomaly'] = df['message'].apply(detect_pattern_anomalies)\n",
    "    detect_statistical_outliers(df)\n",
    "    df['statistical_anomaly'] = df['statistical_anomaly'].astype(float)\n",
    "    detect_clustering_anomalies(df)  \n",
    "    df['clustering_anomaly'] = df['clustering_anomaly'].astype(float)\n",
    "\n",
    "    weights = {\n",
    "        'keyword_anomaly': 0.50,\n",
    "        'pattern_anomaly': 0.25,\n",
    "        'statistical_anomaly': 0.10,\n",
    "        'clustering_anomaly': 0.15\n",
    "    }\n",
    "\n",
    "    #Calculate weights sum of anomaly categories\n",
    "    df['combined_anomaly'] = (\n",
    "        df['keyword_anomaly'] * weights['keyword_anomaly'] +\n",
    "        df['pattern_anomaly'] * weights['pattern_anomaly'] +\n",
    "        df['statistical_anomaly'] * weights['statistical_anomaly'] +\n",
    "        df['clustering_anomaly'] + weights['clustering_anomaly']\n",
    "    )\n",
    "\n",
    "\n",
    "    df['combined_anomaly'] = df['combined_anomaly'].clip(0, 1)\n",
    "    \n",
    "    # Contextual anomaly detection\n",
    "    detect_sequential_anomalies(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse SSH logs\n",
    "def parse_ssh_log(line):\n",
    "    match = re.match(r'^(?P<date>\\w+ \\d+ \\d+:\\d+:\\d+) (?P<source>\\w+) sshd\\[\\d+\\]: (?P<message>.+)$', line)\n",
    "    if match:\n",
    "        date_str = match.group('date')\n",
    "        # SSH logs do not include the year, so assume the current year\n",
    "        date = datetime.strptime(date_str, '%b %d %H:%M:%S')\n",
    "        date = date.replace(year=datetime.now().year)\n",
    "        message = match.group('message')\n",
    "        message_length = len(message)\n",
    "\n",
    "        return date, 'ssh', message, message_length\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse Apache logs\n",
    "def parse_apache_log(line):\n",
    "    match = re.match(r'^\\[(?P<date>.+)\\] \\[(?P<log_level>\\w+)\\] (?P<message>.+)$', line)\n",
    "    if match:\n",
    "        date_str = match.group('date')\n",
    "        try:\n",
    "            # Attempt to parse the date with the year first\n",
    "            date = datetime.strptime(date_str, '%a %b %d %H:%M:%S %Y')\n",
    "        except ValueError:\n",
    "            # If year is not present, assume the current year\n",
    "            date = datetime.strptime(date_str, '%a %b %d %H:%M:%S')\n",
    "            date = date.replace(year=datetime.now().year)\n",
    "        message = match.group('message')\n",
    "        message_length = len(message)\n",
    "\n",
    "        return date, 'apache', message, message_length\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Path to log files\n",
    "log_files = {\n",
    "    'ssh': '../archive-4/SSH.log',\n",
    "    'apache': '../archive-4/Apache.log',\n",
    "}\n",
    "\n",
    "#Parsing functions\n",
    "parsers = {\n",
    "    'ssh': parse_ssh_log,\n",
    "    'apache': parse_apache_log,\n",
    "}\n",
    "\n",
    "\n",
    "#Merging all log files\n",
    "all_logs = []\n",
    "for log_type, file in log_files.items():\n",
    "    with open(file, 'r') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            #if you want to scale dataset, just change the int to whatever scale you want\n",
    "            if count >= 100000:\n",
    "                break\n",
    "            date, lt, message, message_length  = parsers[log_type](line)\n",
    "            if date and lt and message:\n",
    "                all_logs.append([date, log_type, message, message_length])\n",
    "                count += 1\n",
    "\n",
    "\n",
    "#Create DataFrame \n",
    "df = pd.DataFrame(all_logs, columns=['date', 'log_type', 'message', 'message_length'])\n",
    "\n",
    "\n",
    "\n",
    "apply_anomaly_detection(df)\n",
    "\n",
    "#Droping unnecessary columns\n",
    "df.drop(columns=['keyword_anomaly'], inplace=True)\n",
    "df.drop(columns=['pattern_anomaly'], inplace=True)\n",
    "df.drop(columns=['statistical_anomaly'], inplace=True)\n",
    "df.drop(columns=['clustering_anomaly'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#if you have scaled the dataset, try to align the name of the dataset with number of dataset \n",
    "df.to_csv('../merged_dataset/merged_logs100000.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72229e2cb4a4d3f9a66c0f3bbf8721a4f899e8fc91e357b565a53efd017c27d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
